{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "488b27c5-8817-42bd-a075-3c37be00a769",
   "metadata": {},
   "source": [
    "# Resampling\n",
    "\n",
    "The next idea I would like to demonstrate is resampling. This is a method of using a dataset to simmulate the random sampling process on the population in order to obtain estimates for a statistic and its possible varriation. You will see that it is a process that is designed to perform well with larger samples, though truthfully it yields valid results (though with caveats) in any case. It cannot however fix a sampling process that is flawed.\n",
    "\n",
    "---\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "Is the specific case of rebuilding a sample of exactly the same size as the original through resampling with replacement.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "The procedure is to build a new sample by sampling from the dataset with replacement. One then computes the statistic of interest from the new sample and records the value. This process is then repeated multiple times. The result range of computed values forms an analogue of the confidence interval, and one can then reproduce hypothesis tests as well. \n",
    "\n",
    "## Simulating Data\n",
    "\n",
    "A Big issue with educational (and other types of data) is that it is often protected under stringent protocols. A method developed by the Health Insurance industry is effective in creating a data set that merely simmulates the original dataset and in particular does not include the record of any indvidual and so can be used in ways that the original data could not be. One can do this by resampling with replacement from the original data forcing it to fit the parameters of a model to preserve what you think would be interesting features (covariances for instance). \n",
    "\n",
    "It is primarily a tool for when the code for a study is in development allowing one to study the analysis process and tools without having to be as careful with the protected data, with the idea that after things are developed we would then run the code for real on the protected data to obtain the final parameters and models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4418c98a-f674-4d3d-b6d7-9b7f42a0f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = read.csv(\"Datasets/penguins_lter.csv\")\n",
    "gentoo = penguins[penguins[,3]=='Gentoo penguin (Pygoscelis papua)',]\n",
    "gentoo_flipper = na.omit(gentoo[,12])\n",
    "# Note I had to add an na.omit\n",
    "\n",
    "length(gentoo_flipper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce007fa0-931a-4d72-ae0e-cf0bea7bf75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bootstrap this dataset a bunch of times and compute the mean each time:\n",
    "\n",
    "data = c()\n",
    "for (i in 1:50) { \n",
    "  temp = sample(gentoo_flipper, 123, replace = TRUE)\n",
    "  data = c(data, mean(temp))\n",
    "}\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9524d5-1406-46b9-9661-5dd5b74dd10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abe7bb-e7ca-4a8a-968a-b53a56a86cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475d4cc-eff5-4357-a480-9556089245bd",
   "metadata": {},
   "source": [
    "# The Datascience Modeling Process\n",
    "\n",
    "Let's return to the classification of penguins problem. A problem confronting us is to choose a model that gives the *best* prediction of the species of penguins from the inputs that we can manage. More than that, what we want to understand about the model we decide on is what we expect its error to be. \n",
    "\n",
    "Note the danger:  As we increase the complexity of our model we obtain a result which adapts more to the training data. However the training data began as a random sample from a population - and so it contains irreducible errors caused by factors we do not (and in most cases cannot hope to) know.  If our model moves too closely to the training data, it may be responding to these errors rather than actual structure in the population - and we would expect this to then lead to a higher error rate when the model is applied to data it has never seen: the testing set or even worse, new observations. \n",
    "\n",
    "This is called **Overfitting**. \n",
    "\n",
    "Accordingly, much of our effort is going to go into overfitting. We will control for overfitting by repeatedly in the exploration phase of our analysis applying the models we have develop to new testing data that the model has not yet seen. In real applications, with *plenty* of data this process might be nested in multiple layers with a final testing set that is reserved until the final model has been selected and trained. \n",
    "\n",
    "The process of training and testing our model with multiple slices of our dataset is called **Crossvalidation**. And the goal is to find a model that captures true features in the data, avoids the overfitting that arises from responding to irreducible errors in the dataset, and to have a good estimate on the error and its varriation we expect from the final model. \n",
    "\n",
    "## Ethics\n",
    "\n",
    "Note that how much effort one actually puts into this crossvalidation process would be determined by the ethical consideration of how likely errors seem to be, and on what the consequences for errors are. There is a big difference between us playing around with a penguin dataset, and us working for a university on student success data, financial data, or health data. \n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "There are lots of ways to do this. However a common one is to divide the data set randomly into 5 equal size pieces. We will then train our model on four of these pieces using the fifth one as a testing set. We can then repeate that with the same model and parameters five times obtaining an error estimate each time.\n",
    "\n",
    "This will give us five estimates on the error produced by training the model with these parameters on a random training set when the model is applied to data it has not seen. Each individual data point is used four times as a training value and once as a testing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b840ca4-b970-40ef-ba1d-ed0ca0ba2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to remove the rows with missing data; and also pair the data down to just the values we will use\n",
    "data = penguins[,c(3, 12,11)]\n",
    "data = na.omit(data)\n",
    "data$Species = as.factor(data$Species)   # we are going to use decision trees below, and for them the output column needs to be a factor and the inputs numerical or factors.\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6018ca2-b350-43da-9035-7b0d1e079538",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample(nrow(data), nrow(data) )   # Shuffle the row indexes\n",
    "splits = split(x, as.integer((x - 1)/(nrow(data)*0.2)) )   # divide the row indexes up into 5 mostly equal sized pieces\n",
    "shuffles = list()\n",
    "\n",
    "for (i in 1:5) {\n",
    "    shuffles[[i]] = sort(x[splits[[i]]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c302f0-f6ba-43fb-be60-0c09eeab9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list()\n",
    "train = list()\n",
    "\n",
    "for (i in 1:5) {\n",
    "    test[[i]] = data[shuffles[[i]],]\n",
    "    train[[i]] = data[-shuffles[[i]], ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9558b-54df-4539-9437-2a8a16c261f0",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors Again\n",
    "\n",
    "We can now test this with the k-nearest neighbors algorithm we met last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3debc-f2fa-4e9e-bc73-f8cc551fe45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49d239-3bc5-4185-b640-4ef5ca626e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = c()\n",
    "\n",
    "for (i in 1:5) { \n",
    "    pr = knn(train[[i]][,-1],test[[i]][,-1],cl=train[[i]][,1],k=1)\n",
    "    out = c(out, sum(test[[i]][,1]!=pr)/length(test[[i]][,1]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf0c6f-791c-4884-87aa-252c091dd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "out\n",
    "mean(out)\n",
    "sd(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ee1dc-03ea-488e-905b-e3770eab7838",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "It's worth looking at some other models now as well. One of the simplest models we can build is called a decision tree. Decision trees are a list of rules to mkae our classification.\n",
    "\n",
    "They are even easier to explain than the KNN algorithm. And we see examples of them all around us:  For example the placement mechanism UNC uses for our courses is a decision tree: https://www.unco.edu/nhs/mathematical-sciences/placement/results.aspx\n",
    "\n",
    "which tries to classify students into their mathemaitcs courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02eb61f-eec2-409a-abae-53cd2dcd17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tree)\n",
    "library(rpart)\n",
    "library(rpart.plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b54049-6d13-473f-ba3e-676a59cdfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "md <- rpart(Species ~ Flipper.Length..mm. + Culmen.Depth..mm., data=train[[1]] )\n",
    "md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e8489-8c3c-4d6d-802b-6a344a214057",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(\"plot_tree.pdf\") \n",
    "rpart.plot(md, box.palette=\"RdBu\", shadow.col=\"gray\", nn=TRUE )\n",
    "dev.off() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef47fee-8862-4450-8919-170dd72963a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af166d-46ee-45bf-b3e3-5be12f46e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(predict.rpart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ae348-7ce2-4f67-b0a6-1b808f340000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
